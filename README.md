# LLaMA-3.2 Ã— Perfetto Tracing & Profiling  
*Progress snapshot â€“ 16 May 2025*

---

## ğŸ¯ Project Goal  
Visualise **one transformer block** of any LLaMA-3.2 checkpoint with **Perfetto**, exposing every key compute & memory step (dot-product, multi-pass soft-max, RoPE, layer norms, residual adds, KV-cache I/O, MLP, â€¦) and embedding rich tensor-metadata in the trace.

---

## ğŸ“ Repository Layout

./
â”œâ”€â”€ llama/ # Minimal fork with always-on Perfetto hooks
â”‚ â”œâ”€â”€ generation.py # Text-completion wrapper
â”‚ â”œâ”€â”€ model.py # Transformer + fine-grained tracing
â”‚ â””â”€â”€ tokenizer.py # tiktoken-based
â”œâ”€â”€ example_text_completion.py # Local .pth loader demo
â”œâ”€â”€ load_model_and_simulate_perfetto.py # HF loader + custom trace
â”œâ”€â”€ load_model_from_weight.py # CPU-only .pth loader
â”œâ”€â”€ pytorch_profiler.py # TensorBoard-compatible trace
â”œâ”€â”€ test_trace.py # Tiny MLP sanity-check
â””â”€â”€ setup.sh # One-shot env bootstrap

yaml
Copy
Edit

---

## âœ… Current Progress

| # | Requirement | Status | Notes |
|---|-------------|--------|-------|
| **1** | Load model from Hugging Face **safetensors** and local **`.pth`** shards | **âœ” done** | GGUF loader **TBD** |
| **2** | Auto-generate **Perfetto trace** for one block | **âœ” done** | Script still mocks QÂ·Káµ€ & soft-max timing |
| **3** | Trace **adapts to any model size / dims** | **âœ” done** | Will inherit GGUF once loader added |
| **4** | Annotate each span with **op-type + I/O tensor metadata** | **â–² partial** | LayerNorm, RoPE, embedding to add |

Legend â€” **âœ” done**, **â–² partial**, **âœ— todo**

---

## ğŸ”§ Quick Start

```bash
# 1  Create / activate a Python env, then:
bash setup.sh          # installs transformers, tensorboard, â€¦

# 2  Sanity-check: local .pth shards text-completion
python example_text_completion.py

# 3  Generate Perfetto JSON trace on Hugging Face checkpoint (layer 0)
python load_model_and_simulate_perfetto.py
# â†’ writes   llama_block_trace_<model>_<timestamp>.json
# â†’ open trace.perfetto.dev and drop the file

# 4  Optional: TensorBoard profile (HF model)
python pytorch_profiler.py
tensorboard --logdir ./log/hf_llama_block_trace
ğŸš§ Next Steps
GGUF loader â€“ integrate llama_cpp / ggml bindings for quantised CPU models.

Replace placeholders with real compute â€“ call actual matmuls & soft-max to capture true durations.

Trace completeness â€“ add tracks & metadata for token embedding, RoPE, LayerNorm, residual adds.

Packaging â€“ turn scripts into a small CLI:
python -m llama_trace --model meta-llama/... --layer 0.

âœï¸ Contributing
PRs welcome â€” particularly for GGUF support and fuller op-coverage.
Run black + ruff before submitting.